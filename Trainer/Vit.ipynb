{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43ac0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092d5a1",
   "metadata": {},
   "source": [
    "### Load Model and Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df52994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb878f75270d4ff2b7e01571dbef8055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750196064ab442e786e44b87adf3eb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c4774aa9e8403b86f9000e46a11b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seono/anaconda3/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1916e",
   "metadata": {},
   "source": [
    "### Get Embeddings & Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5644fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(image: Image.Image):\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.pooler_output[0]  # or mean pooling\n",
    "        return F.normalize(embedding, dim=0).cpu()\n",
    "\n",
    "def cosine_sim(e1, e2):\n",
    "    return F.cosine_similarity(e1.unsqueeze(0), e2.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558e5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_2(image_path_1, image_path_2):\n",
    "    image1 = Image.open(image_path_1).convert(\"RGB\")\n",
    "    image2 = Image.open(image_path_2).convert(\"RGB\")\n",
    "    return  cosine_sim(get_embedding(image1), get_embedding(image2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35b67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(image):\n",
    "    iamge = Image.open(image).convert(\"RGB\")\n",
    "    return get_embedding(iamge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05fcd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get(\"patch_5.jpg\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be044542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
